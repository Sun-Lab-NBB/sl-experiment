"""This module provides the methods used to preprocess mesoscope data after acquisition. The primary purpose of this
preprocessing is to prepare the data for storage and further processing in the Sun lab data cluster.
"""

from pathlib import Path
from functools import partial
from concurrent.futures import ProcessPoolExecutor, as_completed

from tqdm import tqdm
import numpy as np
import tifffile
import json
from ataraxis_base_utilities import console, ensure_directory_exists

import re
import os
import difflib


def _check_stack_size(file: Path) -> int:
    """Reads the header of the input TIFF file, and if the file is a stack, extracts its size.

    This function is used to both determine the stack size of the processed TIFF files and to exclude non-mesoscope
    TIFFs from processing.

    Notes:
        This function only works with monochrome TIFF stacks generated by the mesoscope. It expects each TIFF file to
        be a stack of 2D frames.

    Args:
        file: The path to the TIFF file to evaluate.

    Returns:
        If the file is a stack, returns the number of frames (pages) in the stack. Otherwise, returns 0 to indicate that
        the file is not a stack.
    """
    with tifffile.TiffFile(str(file)) as tif:
        # Gets number of pages (frames) from tiff header
        n_frames = len(tif.pages)

        # Considers all files with more than one page and a 2-dimensional (monochrome) image as a stack. For these
        # stacks, returns the discovered stack size (number of frames).
        if n_frames > 1 and len(tif.pages[0].shape) == 2:
            return n_frames
        # Otherwise, returns 0 to indicate that the file is not a stack.
        return 0


def _process_stack(tiff_path: Path, output_dir: Path, stack_size: int, remove_sources: bool) -> None:
    """Reads a TIFF stack and saves it as a LERC-compressed stacked TIFF file.

    This is a worker function called by the _extract_frames_from_stack() as it parallelizes stack processing for each
    input directory. Specifically, this function is called for each stack inside each processed TIFF directory. It
    re-compresses the stack using LERC-compression and verifies that each compressed frame matches the original to
    ensure data integrity. This function maintains the input stack dimension and layout.

    Raises:
        RuntimeError: If any extracted frame does not match the original frame stored inside the TIFF stack.

    Args:
        tiff_path: The path to the TIFF stack to process.
        output_dir: The path to the directory for the processed frames.
        stack_size: The size of each TIFF stack.
        remove_sources: Determines whether to remove original TIFF stacks after processing.
    """
    # Loads the stack into RAM
    original_stack = tifffile.imread(str(tiff_path))

    # Uses the base stack name to determine the number and ID of frames inside the stack. Specifically, uses the stack
    # number and the stack size parameters to determine the exact position of each extracted frame in the overall
    # sequence saved over multiple stacks. This is used to determine the range of frames stored in this particular
    # stack
    base_name = tiff_path.stem
    session_sequence: str = base_name.split("__")[-1]
    _, stack_number = session_sequence.split("_")

    # Computes the starting and ending frame number
    actual_stack_size = len(original_stack)
    start_frame = (int(stack_number) - 1) * stack_size + 1
    end_frame = start_frame + actual_stack_size - 1

    # Creates the output path for the compressed stack. Uses 12-digit padding for frame numbering
    output_path = output_dir.joinpath(f"{str(start_frame).zfill(12)}_{str(end_frame).zfill(12)}.tiff")

    tifffile.imwrite(
        output_path,
        original_stack,
        compression="lerc",
        compressionargs={"level": 0.0},  # Lossless compression
        predictor=True,
        resolutionunit="NONE",  # Remove unnecessary metadata
    )

    # Verifies the integrity of the compressed stack
    compressed_stack = tifffile.memmap(output_path)
    if not np.array_equal(compressed_stack, original_stack):
        message = f"Compressed stack does not match the original stack in {tiff_path}."
        console.error(message=message, error=RuntimeError)

    # Remove the original file if requested
    if remove_sources:
        tiff_path.unlink()


def extract_frames_from_stack(
    image_directory: Path, num_processes: int, remove_sources: bool = False, batch: bool = False
) -> None:
    """Loops over all multi-frame TIFF stacks in the input directory and recompresses them using LERC scheme.

    This function is used as a preprocessing step for mesoscope-acquired data that optimizes the size of raw images for
    long-term storage and streaming over the network. To do so, each stack is re-encoded using LERC scheme,
    which achieves ~70% compression ratio, compared to the original frame stacks obtained from the mesoscope.

    Notes:
        This function is specifically calibrated to work with TIFF stacks produced by the scanimage matlab software.
        Critically, these stacks are named using '__' to separate session and stack number from the rest of the
        file name, and the stack number is always found last, e.g.: 'Tyche-A7_2022_01_25_1__00001_00067.tif'. If the
        input TIFF files do not follow this naming convention, the function will not work as expected.

        This function assumes that scanimage buffers frames until the stack_size number of frames is available and then
        saves the frames as a TIFF stack. Therefore, it assumes that the directory contains at most one non-full stack.
        The function uses this assumption when assigning unique frame IDs to extracted frames.

        To optimize runtime efficiency, this function employs multiple processes to work with multiple TIFF at the
        same time. It uses the stack number and stack size as a heuristic to determine which IDs to assign to each
        extracted frame while processing stacks in-parallel to avoid collisions.

    Args:
        image_directory: The directory containing the multi-frame TIFF stacks.
        num_processes: The maximum number of processes to use while processing the directory.
        remove_sources: Determines whether to remove the original TIFF files after they have been processed.
        batch: Determines whether the function is called as part of batch-processing multiple directories. This is used
            to optimize progress reporting to avoid cluttering the terminal.
    """
    # Generates a new 'mesoscope_frames' directory to store extracted .tiff frame files.
    output_dir = image_directory.joinpath("mesoscope_frames")
    ensure_directory_exists(output_dir)

    # Finds all TIFF files in the input directory (non-recursive).
    tiff_files = list(image_directory.glob("*.tif")) + list(image_directory.glob("*.tiff"))

    # Loops over each tiff stack and extracts the number of frames inside each stack. If the file is not a stack,
    # removes it from further processing.
    maximum_stack_size = 0
    for file in tiff_files:
        stack_size = _check_stack_size(file)

        if stack_size == 0:
            tiff_files.remove(file)  # Removes non-stack files from further processing

        # If any stack has a larger size than the current maximum, updates the maximum size
        maximum_stack_size = max(stack_size, maximum_stack_size)

    # If there are TIFFs to process, executes frame extraction
    if len(tiff_files) > 0:
        # Uses partial to bind the constant arguments
        process_func = partial(
            _process_stack, output_dir=output_dir, stack_size=maximum_stack_size, remove_sources=remove_sources
        )

        # Processes each tiff stack in parallel
        with ProcessPoolExecutor(max_workers=num_processes) as executor:
            # Submits all tasks
            future_to_file = {executor.submit(process_func, file): file for file in tiff_files}

            if not batch:
                # Shows progress with tqdm when not in batch mode
                with tqdm(total=len(tiff_files), desc="Processing TIFF stacks", unit="files") as pbar:
                    for future in as_completed(future_to_file):
                        future.result()  # Gets result to ensure completion
                        pbar.update(1)
            else:
                # For batch mode, processes without progress tracking
                [future.result() for future in as_completed(future_to_file)]


def extract_metadata_to_json(input_tif, output_json='metadata.json'):
    """
    Extract all TIFF tags from the first page of 'input_tif' and save
    them as JSON in 'output_json'.
    """
    with tifffile.TiffFile(input_tif) as tf:
        # Access the first page
        first_page = tf.pages[0]

        # Prepare a dictionary: {tag_name: tag_value_as_string}
        metadata = {}
        for code, tifftag in first_page.tags.items():
            # tifftag.name is a human-readable name (e.g. "ImageDescription", "Software")
            tag_name = tifftag.name

            # tifftag.value can be str, bytes, int, float, list, etc.
            value = tifftag.value
            if isinstance(value, bytes):
                try:
                    value = value.decode("utf-8", errors="replace")
                except:
                    pass  # leave as bytes if decoding fails

            # Convert to a standard Python type suitable for JSON
            metadata[tag_name] = value

    # Write all tag/value pairs to a JSON file
    with open(output_json, "w", encoding="utf-8") as f:
        # indent=4 for readability; ensure_ascii=False so UTF-8 characters remain
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    print(f"Extracted metadata from {input_tif} (first page) to {output_json}")


def export_metadata_only(input_tif, output_tif='metadata.tif'):
    """
    Extract all tags from the first page of `input_tif` and store them in a new
    TIFF file `output_tif`. The new file contains a minimal 1×1 dummy image plus
    the copied metadata tags.

    Parameters
    ----------
    input_tif : str
        Path to the original multi-page TIFF file (or single-page TIFF).
    output_tif : str, optional
        Path for the output "metadata-only" TIFF. Defaults to 'metadata.tif'.
    """

    # 1) Open the original TIFF and gather tags from the first page
    with tifffile.TiffFile(input_tif) as tf:
        first_page = tf.pages[0]  # get the first page

        # We'll collect extratags that are NOT automatically managed by TiffWriter
        # (ImageWidth, ImageLength, BitsPerSample, etc. are set by tifffile itself)
        auto_tags = {
            256,  # ImageWidth
            257,  # ImageLength
            258,  # BitsPerSample
            259,  # Compression
            262,  # Photometric
            273,  # StripOffsets
            277,  # SamplesPerPixel
            278,  # RowsPerStrip
            279,  # StripByteCounts
            280,  # MinSampleValue
            282,  # XResolution
            283,  # YResolution
            284,  # PlanarConfiguration
            296,  # ResolutionUnit
            305,  # Software
            306,  # DateTime
            315,  # Artist
            330,  # SubIFDs
            339,  # SampleFormat
            531,  # ReferenceBlackWhite
        }

        extratags = []
        for code, tifftag in first_page.tags.items():
            if code in auto_tags:
                # Skip tags that TiffWriter will overwrite or that conflict with new image shape
                continue

            # TiffTag attributes: .name, .value, .count, .dtype
            extratags.append((code, tifftag.dtype, tifftag.count, tifftag.value, False))

    # 2) Create a minimal 1x1 dummy image
    dummy_img = np.zeros((1, 1), dtype=np.uint8)

    # 3) Write the new TIFF with extratags
    with tifffile.TiffWriter(output_tif, bigtiff=True) as tw:
        # We let TiffWriter automatically set the standard tags for a 1×1, 8-bit image
        # and we attach our extratags to store the original metadata
        tw.write(dummy_img, extratags=extratags)

    print(f"Extracted metadata from first page of '{input_tif}' into '{output_tif}'.")


def generate_ops_from_metadata(
    metadata_json_path,
    output_ops_json="ops.json"
):
    """
    Generate ops.json from the metadata in metadata_json_path (extracted TIFF tags).
    If `tiff_path` is provided and `stack_first_frame=True`, we will load the first
    frame to get the actual 'height' for computing flyback frames.

    If you already know the height or don't need the flyback logic, you can omit it
    or pass in a known value some other way.
    """

    # ----------------------------------------------------------------
    # 0) Load the extracted metadata from JSON
    #    The JSON is assumed to have keys like "Software", "Artist", etc.
    # ----------------------------------------------------------------
    with open(metadata_json_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    # For convenience, we'll do:
    software_tag = metadata.get("Software", "")
    artist_tag = metadata.get("Artist", "")

    # ----------------------------------------------------------------
    # 1) Determine the image height (stack_height) from the tags.
    # ----------------------------------------------------------------
    stack_height = metadata.get("ImageLength", None)

    path_to_use = metadata_json_path
    root = os.path.dirname(os.path.abspath(path_to_use))

    # ----------------------------------------------------------------
    # 2) Parse the frame rate `fs` from the software line
    #    - The MATLAB code searches for "SI.hRoiManager.scanVolumeRate"
    # ----------------------------------------------------------------
    fs = 4  # fallback
    # Split software_tag by lines
    lines_software = software_tag.splitlines()
    for line in lines_software:
        # e.g., "SI.hRoiManager.scanVolumeRate = 5"
        if "SI.hRoiManager.scanVolumeRate" in line:
            parts = line.split('=')
            if len(parts) > 1:
                try:
                    fs = float(parts[1].strip())
                except ValueError:
                    pass
        # e.g., "SI.hFastZ.userZs = [0 15 30]"
        if "SI.hFastZ.userZs" in line:
            # parse array
            match_z = re.search(r'\[(.*)\]', line)
            if match_z:
                zs_str = match_z.group(1).strip()
                zs_vals = zs_str.split()
                nplanes = len(zs_vals)
            else:
                nplanes = 1
        else:
            nplanes = 1

    # ----------------------------------------------------------------
    # 3) Parse the "Artist" tag as JSON to get rois
    #    - The MATLAB code extracts sub-fields from artist.RoiGroups.imagingRoiGroup.rois
    # ----------------------------------------------------------------
    # The MATLAB snippet does:
    #   artist_info = artist_info(1:find(artist_info == '}', 1, 'last'));
    #   artist = jsondecode(artist_info);
    #   si_rois = artist.RoiGroups.imagingRoiGroup.rois;
    # We do something similar in Python:

    si_rois = []
    # try to find the trailing brace if there's extra content
    match = re.search(r'(.*\})', artist_tag)
    if match:
        trimmed_json = match.group(1)
    else:
        trimmed_json = artist_tag  # fallback to entire string
    try:
        artist_dict = json.loads(trimmed_json)
        si_rois = artist_dict["RoiGroups"]["imagingRoiGroup"]["rois"]
    except Exception:
        si_rois = []

    nrois = len(si_rois)

    Ly = []
    Lx = []
    cXY = []
    szXY = []

    for r in si_rois:
        # rois[k].scanfields(1).pixelResolutionXY => [W, H]
        sf = r["scanfields"]
        # Sometimes it's a list; sometimes just one dict
        sf0 = sf[0] if isinstance(sf, list) else sf

        pixel_res_xy = sf0.get("pixelResolutionXY", [0, 0])
        center_xy    = sf0.get("centerXY", [0, 0])
        size_xy      = sf0.get("sizeXY", [0, 0])

        # MATLAB does:
        # Ly(k,1) = .pixelResolutionXY(2)
        # Lx(k,1) = .pixelResolutionXY(1)
        Ly.append(pixel_res_xy[1])
        Lx.append(pixel_res_xy[0])

        # cXY(k, [2,1]) = .centerXY
        # szXY(k, [2,1]) = .sizeXY
        cXY.append([center_xy[1], center_xy[0]])
        szXY.append([size_xy[1], size_xy[0]])

    Ly = np.array(Ly)
    Lx = np.array(Lx)
    cXY = np.array(cXY)
    szXY = np.array(szXY)

    # cXY = cXY - szXY/2; cXY = cXY - min(cXY,[],1);
    if len(cXY) > 0:
        cXY = cXY - (szXY / 2.0)
        cXY = cXY - np.min(cXY, axis=0)

    # mu = median([Ly, Lx]./szXY,1);
    # => we can do a column stack of (Ly, Lx), then elementwise divide by szXY
    if len(Ly) > 0:
        stack_lylx = np.column_stack((Ly, Lx))
        safe_szxy = np.copy(szXY)
        safe_szxy[safe_szxy == 0] = 1e-9
        ratio = stack_lylx / safe_szxy
        mu = np.median(ratio, axis=0)  # shape (2,)
    else:
        mu = np.array([1, 1], dtype=float)

    # imin = cXY .* mu
    if len(cXY) > 0:
        imin = cXY * mu
    else:
        imin = np.zeros((0, 2))

    # deduce flyback frames from the most filled z-plane
    # (the MATLAB code uses size(stack,1) for total # of rows.)
    if stack_height is not None and len(Ly) > 0:
        n_rows_sum = np.sum(Ly)
        if nrois > 1:
            n_flyback = (stack_height - n_rows_sum) / (nrois - 1)
        else:
            n_flyback = 0
    else:
        # fallback
        n_rows_sum = 0
        n_flyback = 0

    # irow = [0 cumsum(Ly'+n_flyback)]
    # irow(end) = [];
    # irow(2,:) = irow(1,:) + Ly';
    irow_starts = [0]
    irow_ends = []
    for val in Ly:
        irow_ends.append(irow_starts[-1] + val)
        irow_starts.append(irow_starts[-1] + val + n_flyback)
    # remove last start if it overshoots
    if len(irow_starts) > len(irow_ends):
        irow_starts.pop()

    # ----------------------------------------------------------------
    # 4) Construct the "data" structure, matching the MATLAB logic
    # ----------------------------------------------------------------
    data = {}
    data["fs"] = fs
    data["nplanes"] = nplanes
    data["nrois"] = nrois
    data["mesoscan"] = 1 if nrois > 1 else 0
    data["diameter"] = [6, 9]
    data["num_workers_roi"] = 5
    data["keep_movie_raw"] = 0
    data["delete_bin"] = 1
    data["batch_size"] = 1000
    data["nimg_init"] = 400
    data["tau"] = 2.0
    data["combined"] = 1
    data["nonrigid"] = 1

    if data["mesoscan"]:
        data["dx"] = []
        data["dy"] = []
        data["lines"] = []
        for i, (start_val, end_val) in enumerate(zip(irow_starts, irow_ends)):
            data["lines"].append(list(range(int(start_val), int(end_val))))
            if i < len(imin):
                data["dx"].append(int(imin[i, 1]))
                data["dy"].append(int(imin[i, 0]))
            else:
                data["dx"].append(0)
                data["dy"].append(0)

    # ----------------------------------------------------------------
    # 5) Derive data_path and save_path0 from `root`
    #    The MATLAB code does some string manipulation for that
    # ----------------------------------------------------------------
    # We'll just store `root` as data_path[0] for simplicity:
    # You can adapt if you want different logic
    normalized_root = os.path.abspath(root)
    data["data_path"] = [normalized_root]

    # The MATLAB code modifies the drive letter to "G:". We'll do a simple approach
    # or you can replicate exactly if you want:
    data["save_path0"] = f"G:{os.path.abspath(root)[2:]}"  # e.g. G:\DATA\...
    # Or just store the same root in case you don't want to alter the drive:
    # data["save_path0"] = normalized_root

    # ----------------------------------------------------------------
    # 6) Write ops.json
    # ----------------------------------------------------------------
    output_path = os.path.join(root, output_ops_json)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

    print(f"Saved ops.json to {output_path}")


def diff_ops_files_json(ops_path1, ops_path2):
    with open(ops_path1, 'r', encoding='utf-8') as f1:
        data1 = json.load(f1)
    with open(ops_path2, 'r', encoding='utf-8') as f2:
        data2 = json.load(f2)

    # Convert back to JSON with sorted keys, so differences are consistent
    json1 = json.dumps(data1, sort_keys=True, indent=2)
    json2 = json.dumps(data2, sort_keys=True, indent=2)

    # Compare line by line
    diff = difflib.unified_diff(
        json1.splitlines(),
        json2.splitlines(),
        fromfile=ops_path1,
        tofile=ops_path2,
        lineterm=''
    )

    # Print the diff
    printed = False
    for line in diff:
        printed = True
        print(line)
    if not printed:
        print("No differences found! The JSON content is identical.")


if __name__ == "__main__":
    input_file = "/media/Data/Tyche-A2/2022_01_25/1/Tyche-A7_2022_01_25_1__00001_00008.tif"
    extract_metadata_to_json(input_file, "/media/Data/Tyche-A2/2022_01_25/1/metadata.json")
    export_metadata_only(input_file, "/media/Data/Tyche-A2/2022_01_25/1/metadata.tiff")

    generate_ops_from_metadata(
        metadata_json_path="/media/Data/Tyche-A2/2022_01_25/1/metadata.json",
        output_ops_json="ops.json"
    )

    # ops_1 = "/media/Data/2022_01_25/1/ops.json"
    # ops_2 = "/media/Data/Tyche-A2/2022_01_25/1/ops.json"
    # diff_ops_files_json(ops_1, ops_2)
